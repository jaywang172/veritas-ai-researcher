#!/usr/bin/env python3
"""
Veritas LLM Configuration Framework
æ¨¡çµ„åŒ–ã€å¯é…ç½®çš„LLMæ¶æ§‹ï¼Œæ”¯æ´ç‚ºä¸åŒAgenté¸æ“‡æœ€é©åˆçš„æ¨¡å‹
"""

import os
from typing import Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

try:
    from langchain_openai import ChatOpenAI
except ImportError:
    # æ¸¬è©¦æ¨¡å¼ï¼šå‰µå»ºæ¨¡æ“¬çš„ChatOpenAIé¡
    class ChatOpenAI:
        def __init__(self, model="gpt-4", temperature=0.1, **kwargs):
            self.model_name = model
            self.temperature = temperature
            self.kwargs = kwargs
        
        def __repr__(self):
            return f"ChatOpenAI(model={self.model_name}, temperature={self.temperature})"

# æ”¯æ´çš„LLMæä¾›å•†
class LLMProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    LOCAL = "local"

# æ¨¡å‹æ•ˆèƒ½ç´šåˆ¥
class ModelTier(Enum):
    BASIC = "basic"        # åŸºç¤ä»»å‹™ï¼šæ ¼å¼è½‰æ›ã€è³‡æ–™æ“·å–
    STANDARD = "standard"  # æ¨™æº–ä»»å‹™ï¼šåˆ†æã€è¦åŠƒ
    ADVANCED = "advanced"  # é«˜ç´šä»»å‹™ï¼šå‰µä½œã€ç·¨è¼¯
    PREMIUM = "premium"    # é ‚ç´šä»»å‹™ï¼šè¤‡é›œæ¨ç†ã€å‰µæ–°

@dataclass
class LLMConfig:
    """LLMé…ç½®é¡åˆ¥"""
    provider: LLMProvider
    model_name: str
    temperature: float = 0.1
    max_tokens: Optional[int] = None
    tier: ModelTier = ModelTier.STANDARD
    cost_per_token: float = 0.0  # æ¯tokenæˆæœ¬ï¼ˆç”¨æ–¼æˆæœ¬æœ€ä½³åŒ–ï¼‰
    description: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        return {
            "provider": self.provider.value,
            "model_name": self.model_name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "tier": self.tier.value,
            "cost_per_token": self.cost_per_token,
            "description": self.description
        }

# é å®šç¾©çš„LLMé…ç½®æ¨¡æ¿ï¼ˆåŸºæ–¼æœ€æ–°OpenAIå®šåƒ¹ï¼‰
LLM_CONFIGS = {
    # æœ€æ–°GPT-5ç³»åˆ—æ¨¡å‹
    "gpt-5": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-5",
        temperature=0.1,
        tier=ModelTier.PREMIUM,
        cost_per_token=0.002625,  # $1.25 input + $10 output (å‡è¨­1:3æ¯”ä¾‹)
        description="æœ€æ–°æ——è‰¦æ¨¡å‹ï¼Œé ‚ç´šæ¨ç†å’Œå‰µä½œèƒ½åŠ›"
    ),
    
    "gpt-5-mini": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-5-mini",
        temperature=0.1,
        tier=ModelTier.ADVANCED,
        cost_per_token=0.000625,  # $0.25 input + $2 output (å‡è¨­1:3æ¯”ä¾‹)
        description="GPT-5ç²¾ç°¡ç‰ˆï¼Œé«˜æ€§åƒ¹æ¯”çš„é«˜ç´šæ¨¡å‹"
    ),
    
    "gpt-5-nano": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-5-nano",
        temperature=0.1,
        tier=ModelTier.STANDARD,
        cost_per_token=0.000175,  # $0.05 input + $0.4 output (å‡è¨­1:3æ¯”ä¾‹)
        description="GPT-5è¼•é‡ç‰ˆï¼Œé©åˆå¤§è¦æ¨¡éƒ¨ç½²"
    ),
    
    # GPT-4.1ç³»åˆ—ï¼ˆæœ€æ–°å®šåƒ¹ï¼‰
    "gpt-4.1": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-4.1",
        temperature=0.1,
        tier=ModelTier.ADVANCED,
        cost_per_token=0.0025,  # $2.0 input + $8 output (å‡è¨­1:3æ¯”ä¾‹)
        description="GPT-4.1æ¨™æº–ç‰ˆï¼Œå¹³è¡¡æ€§èƒ½èˆ‡æˆæœ¬"
    ),
    
    "gpt-4.1-mini": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-4.1-mini",
        temperature=0.1,
        tier=ModelTier.STANDARD,
        cost_per_token=0.0005,  # $0.4 input + $1.6 output (å‡è¨­1:3æ¯”ä¾‹)
        description="GPT-4.1ç²¾ç°¡ç‰ˆï¼Œç¶“æ¿Ÿå¯¦ç”¨çš„é¸æ“‡"
    ),
    
    "gpt-4.1-nano": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-4.1-nano",
        temperature=0.1,
        tier=ModelTier.BASIC,
        cost_per_token=0.000125,  # $0.1 input + $0.4 output (å‡è¨­1:3æ¯”ä¾‹)
        description="GPT-4.1è¼•é‡ç‰ˆï¼Œæœ€ç¶“æ¿Ÿçš„GPT-4.1é¸é …"
    ),
    
    # GPT-4oç³»åˆ—
    "gpt-4o": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-4o",
        temperature=0.1,
        tier=ModelTier.PREMIUM,
        cost_per_token=0.00375,  # $2.5 input + $10 output (å‡è¨­1:3æ¯”ä¾‹)
        description="GPT-4oå¤šæ¨¡æ…‹æ¨¡å‹ï¼Œæ”¯æ´åœ–åƒå’Œæ–‡å­—"
    ),
    
    "gpt-4o-mini": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-4o-mini",
        temperature=0.1,
        tier=ModelTier.BASIC,
        cost_per_token=0.0002625,  # $0.15 input + $0.6 output (å‡è¨­1:3æ¯”ä¾‹)
        description="GPT-4oç²¾ç°¡ç‰ˆï¼Œå¤šæ¨¡æ…‹åŠŸèƒ½çš„ç¶“æ¿Ÿé¸æ“‡"
    ),
    
    # Oç³»åˆ—æ¨ç†æ¨¡å‹
    "o3": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="o3",
        temperature=0.1,
        tier=ModelTier.PREMIUM,
        cost_per_token=0.003,  # $2.0 input + $8 output (å‡è¨­1:3æ¯”ä¾‹)
        description="O3æ¨ç†æ¨¡å‹ï¼Œå°ˆé•·è¤‡é›œé‚è¼¯æ¨ç†"
    ),
    
    "o3-mini": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="o3-mini",
        temperature=0.1,
        tier=ModelTier.ADVANCED,
        cost_per_token=0.001925,  # $1.1 input + $4.4 output (å‡è¨­1:3æ¯”ä¾‹)
        description="O3ç²¾ç°¡ç‰ˆï¼Œå¹³è¡¡æ¨ç†èƒ½åŠ›èˆ‡æˆæœ¬"
    ),
    
    # ç¶“å…¸æ¨¡å‹ä¿ç•™
    "gpt-3.5-turbo": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-3.5-turbo",
        temperature=0.1,
        tier=ModelTier.BASIC,
        cost_per_token=0.000625,  # $0.5 input + $1.5 output (å‡è¨­1:3æ¯”ä¾‹)
        description="ç¶“å…¸é«˜æ€§åƒ¹æ¯”æ¨¡å‹ï¼Œé©åˆåŸºç¤ä»»å‹™"
    ),
    
    # æœªä¾†æ“´å±•çš„é…ç½®
    "claude-3": LLMConfig(
        provider=LLMProvider.ANTHROPIC,
        model_name="claude-3-sonnet-20240229",
        temperature=0.1,
        tier=ModelTier.ADVANCED,
        cost_per_token=0.000015,
        description="Anthropic Claude 3ï¼Œæ“…é•·æ¨ç†å’Œåˆ†æ"
    ),
}

# Agentå°ˆç”¨LLMé…ç½®æ˜ å°„ï¼ˆåŸºæ–¼æœ€æ–°æ¨¡å‹ï¼‰
AGENT_LLM_MAPPING = {
    "literature_scout": "gpt-4o-mini",        # æ–‡ç»æœé›†ï¼šåŸºç¤ä»»å‹™ï¼Œé‡è¦–é€Ÿåº¦å’Œæˆæœ¬
    "synthesizer": "gpt-4.1-mini",            # ç ”ç©¶åˆ†æï¼šæ¨™æº–ä»»å‹™ï¼Œéœ€è¦æº–ç¢ºæ€§
    "outline_planner": "o3-mini",             # å¤§ç¶±è¦åŠƒï¼šé«˜ç´šä»»å‹™ï¼Œéœ€è¦é‚è¼¯æ€ç¶­
    "academic_writer": "gpt-5-mini",          # å­¸è¡“å¯«ä½œï¼šé ‚ç´šä»»å‹™ï¼Œéœ€è¦å‰µé€ åŠ›
    "editor": "gpt-5",                       # ç·¨è¼¯å¯©é–±ï¼šé ‚ç´šä»»å‹™ï¼Œéœ€è¦èªè¨€ç²¾é€š
    "citation_formatter": "gpt-4.1-mini",    # å¼•æ–‡æ ¼å¼åŒ–ï¼šéœ€è¦æº–ç¢ºçš„URLè§£æå’Œæ ¼å¼åŒ–
    "computational_scientist": "o3",          # è¨ˆç®—ç§‘å­¸ï¼šé ‚ç´šä»»å‹™ï¼Œéœ€è¦é‚è¼¯æ¨ç†å’Œä»£ç¢¼ç”Ÿæˆ
    "project_manager": "o3",                 # å°ˆæ¡ˆç®¡ç†ï¼šé ‚ç´šä»»å‹™ï¼Œéœ€è¦ç­–ç•¥æ€ç¶­å’Œæ±ºç­–èƒ½åŠ›
}

class LLMFactory:
    """LLMå·¥å» é¡åˆ¥ï¼Œè² è²¬å»ºç«‹å’Œç®¡ç†LLMå¯¦ä¾‹"""
    
    @staticmethod
    def create_llm(config_name: str, **overrides) -> ChatOpenAI:
        """
        æ ¹æ“šé…ç½®åç¨±å»ºç«‹LLMå¯¦ä¾‹
        
        Args:
            config_name: é…ç½®åç¨±
            **overrides: è¦†è“‹é è¨­é…ç½®çš„åƒæ•¸
            
        Returns:
            ChatOpenAIå¯¦ä¾‹
        """
        if config_name not in LLM_CONFIGS:
            raise ValueError(f"æœªçŸ¥çš„LLMé…ç½®: {config_name}")
        
        config = LLM_CONFIGS[config_name]
        
        # æ‡‰ç”¨è¦†è“‹åƒæ•¸
        model_name = overrides.get("model_name", config.model_name)
        temperature = overrides.get("temperature", config.temperature)
        max_tokens = overrides.get("max_tokens", config.max_tokens)
        
        # æŸäº›æ¨¡å‹ä¸æ”¯æ´è‡ªå®šç¾©æº«åº¦ï¼Œä½¿ç”¨é»˜èªå€¼ 1.0
        if any(prefix in model_name for prefix in ["o3", "o1", "gpt-5"]):
            temperature = 1.0
        
        # ç›®å‰ä¸»è¦æ”¯æ´OpenAIï¼Œæœªä¾†å¯æ“´å±•å…¶ä»–æä¾›å•†
        if config.provider == LLMProvider.OPENAI:
            llm_params = {
                "model": model_name,
            }
            # è¨­å®šæº«åº¦åƒæ•¸
            llm_params["temperature"] = temperature
            if max_tokens:
                llm_params["max_tokens"] = max_tokens
                
            return ChatOpenAI(**llm_params)
        else:
            raise NotImplementedError(f"æš«ä¸æ”¯æ´æä¾›å•†: {config.provider}")
    
    @staticmethod
    def create_agent_llm(agent_type: str, **overrides) -> ChatOpenAI:
        """
        ç‚ºç‰¹å®šAgenté¡å‹å‰µå»ºå„ªåŒ–çš„LLMå¯¦ä¾‹
        
        Args:
            agent_type: Agenté¡å‹
            **overrides: è¦†è“‹é»˜èªé…ç½®çš„åƒæ•¸
            
        Returns:
            ChatOpenAIå¯¦ä¾‹
        """
        if agent_type not in AGENT_LLM_MAPPING:
            print(f"âš ï¸ æœªæ‰¾åˆ°Agent '{agent_type}' çš„å°ˆç”¨é…ç½®ï¼Œä½¿ç”¨é»˜èªé…ç½®")
            config_name = "gpt-4.1"
        else:
            config_name = AGENT_LLM_MAPPING[agent_type]
        
        return LLMFactory.create_llm(config_name, **overrides)
    
    @staticmethod
    def get_model_info(config_name: str) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹é…ç½®ä¿¡æ¯"""
        if config_name not in LLM_CONFIGS:
            return {}
        
        config = LLM_CONFIGS[config_name]
        return config.to_dict()
    
    @staticmethod
    def list_available_models() -> Dict[str, str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®"""
        return {name: config.description for name, config in LLM_CONFIGS.items()}
    
    @staticmethod
    def estimate_cost(config_name: str, estimated_tokens: int) -> float:
        """ä¼°ç®—ä½¿ç”¨æˆæœ¬"""
        if config_name not in LLM_CONFIGS:
            return 0.0
        
        config = LLM_CONFIGS[config_name]
        return config.cost_per_token * estimated_tokens
    
    @staticmethod
    def get_cost_optimized_config(agent_type: str, complexity_level: str = "auto") -> str:
        """
        æ ¹æ“šä»»å‹™è¦†é›œåº¦å’Œæˆæœ¬è€ƒæ…®é¸æ“‡æœ€å„ªé…ç½®
        
        Args:
            agent_type: Agenté¡å‹
            complexity_level: è¦†é›œåº¦ç´šåˆ¥ ("low", "medium", "high", "auto")
            
        Returns:
            é…ç½®åç¨±
        """
        # è‡ªå‹•æ ¹æ“šAgenté¡å‹åˆ¤æ–·è¦†é›œåº¦
        if complexity_level == "auto":
            complexity_map = {
                "literature_scout": "low",         # æœç´¢ä»»å‹™ç›¸å°ç°¡å–®
                "synthesizer": "medium",           # åˆ†æéœ€è¦ä¸€å®šèƒ½åŠ›
                "outline_planner": "high",         # è¦åŠƒéœ€è¦é«˜ç´šæ¨ç†
                "academic_writer": "high",         # å¯«ä½œéœ€è¦å‰µé€ åŠ›
                "editor": "high",                  # ç·¨è¼¯éœ€è¦èªè¨€ç²¾é€š
                "citation_formatter": "low",       # æ ¼å¼åŒ–ç›¸å°æ©Ÿæ¢°
                "computational_scientist": "high"  # ä»£ç¢¼ç”Ÿæˆå’Œæ•¸æ“šåˆ†æéœ€è¦é«˜ç´šæ¨ç†
            }
            complexity_level = complexity_map.get(agent_type, "medium")
        
        # æ ¹æ“šè¦†é›œåº¦é¸æ“‡æœ€ç¶“æ¿Ÿçš„é…ç½®
        if complexity_level == "low":
            return "gpt-3.5-turbo"  # æœ€ç¶“æ¿Ÿ
        elif complexity_level == "medium":
            return "gpt-4.1"        # å¹³è¡¡
        else:  # high
            return "gpt-4-turbo"    # æœ€å¼·èƒ½åŠ›
    
    @staticmethod
    def create_budget_conscious_llm(agent_type: str, budget_tier: str = "balanced") -> ChatOpenAI:
        """
        å‰µå»ºé ç®—å‹å¥½çš„LLMå¯¦ä¾‹
        
        Args:
            agent_type: Agenté¡å‹
            budget_tier: é ç®—ç´šåˆ¥ ("economy", "balanced", "premium")
            
        Returns:
            ChatOpenAIå¯¦ä¾‹
        """
        budget_configs = {
            "economy": {
                "literature_scout": "gpt-4.1-nano",
                "synthesizer": "gpt-4.1-nano", 
                "outline_planner": "gpt-4.1-mini",
                "academic_writer": "gpt-5-nano",
                "editor": "gpt-5-nano",
                "citation_formatter": "gpt-4o-mini",
                "computational_scientist": "gpt-4.1-mini"
            },
            "balanced": AGENT_LLM_MAPPING,  # ä½¿ç”¨é»˜èªé…ç½®
            "premium": {
                "literature_scout": "gpt-4o",
                "synthesizer": "gpt-5-mini",
                "outline_planner": "o3",
                "academic_writer": "gpt-5",
                "editor": "gpt-5",
                "citation_formatter": "gpt-4o",
                "computational_scientist": "o3"
            }
        }
        
        config_name = budget_configs[budget_tier].get(agent_type, "gpt-4.1")
        return LLMFactory.create_llm(config_name)
    
    @staticmethod
    def compare_configurations() -> Dict[str, Any]:
        """æ¯”è¼ƒä¸åŒé…ç½®çš„æˆæœ¬å’Œæ€§èƒ½"""
        comparison = {}
        estimated_tokens = 1000  # å‡è¨­æ¯å€‹ä»»å‹™1000å€‹token
        
        for tier in ["economy", "balanced", "premium"]:
            total_cost = 0
            tier_config = {}
            
            for agent_type in AGENT_LLM_MAPPING.keys():
                if tier == "economy":
                    config_name = LLMFactory.get_cost_optimized_config(agent_type, "low")
                elif tier == "premium":
                    config_name = LLMFactory.get_cost_optimized_config(agent_type, "high")
                else:
                    config_name = AGENT_LLM_MAPPING[agent_type]
                
                cost = LLMFactory.estimate_cost(config_name, estimated_tokens)
                total_cost += cost
                tier_config[agent_type] = {
                    "model": config_name,
                    "cost": cost,
                    "tier": LLM_CONFIGS[config_name].tier.value
                }
            
            comparison[tier] = {
                "total_cost": total_cost,
                "agents": tier_config,
                "cost_savings_vs_premium": 0  # å°‡åœ¨ä¸‹é¢è¨ˆç®—
            }
        
        # è¨ˆç®—ç›¸å°æ–¼premiumçš„æˆæœ¬ç¯€çœ
        premium_cost = comparison["premium"]["total_cost"]
        for tier in comparison:
            if tier != "premium":
                savings = (premium_cost - comparison[tier]["total_cost"]) / premium_cost * 100
                comparison[tier]["cost_savings_vs_premium"] = savings
        
        return comparison

def print_llm_configuration():
    """åˆ—å°ç›®å‰LLMé…ç½®æ¦‚è¦½"""
    print("\nğŸ§  Veritas å¤šå…ƒæ™ºæ…§é…ç½®æ¦‚è¦½")
    print("=" * 70)
    print(f"{'Agenté¡å‹':<20} {'ä½¿ç”¨æ¨¡å‹':<15} {'æ•ˆèƒ½ç´šåˆ¥':<10} {'é ä¼°æˆæœ¬/1K tokens'}")
    print("-" * 70)
    
    total_cost = 0
    for agent_type, config_name in AGENT_LLM_MAPPING.items():
        config = LLM_CONFIGS[config_name]
        cost = LLMFactory.estimate_cost(config_name, 1000)
        total_cost += cost
        print(f"{agent_type:<20} {config_name:<15} {config.tier.value:<10} ${cost:.4f}")
    
    print("-" * 70)
    print(f"{'ç¸½ä¼°ç®—æˆæœ¬ï¼ˆæ¯è¼ªï¼‰':<45} ${total_cost:.4f}")
    print("=" * 70)

def print_budget_comparison():
    """æ‰“å°ä¸åŒé ç®—ç´šåˆ¥çš„æ¯”è¼ƒ"""
    print("\nğŸ’° é ç®—ç´šåˆ¥å°æ¯”åˆ†æ")
    print("=" * 80)
    
    comparison = LLMFactory.compare_configurations()
    
    for tier, data in comparison.items():
        print(f"\nğŸ“Š {tier.upper()} é…ç½®:")
        print(f"   ç¸½æˆæœ¬: ${data['total_cost']:.4f} (æ¯è¼ª)")
        if data['cost_savings_vs_premium'] > 0:
            print(f"   ç¯€çœ: {data['cost_savings_vs_premium']:.1f}% vs Premium")
        print("   Agenté…ç½®:")
        
        for agent, info in data['agents'].items():
            print(f"      {agent:<18}: {info['model']:<15} ({info['tier']} tier)")
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ å»ºè­°:")
    print("   â€¢ Economy: é©åˆå¤§è¦æ¨¡æ‡‰ç”¨æˆ–é ç®—ç·Šå¼µçš„å ´æ™¯")
    print("   â€¢ Balanced: æ¨è–¦é…ç½®ï¼Œæ€§èƒ½èˆ‡æˆæœ¬çš„æœ€ä½³å¹³è¡¡")  
    print("   â€¢ Premium: é©åˆå°è³ªé‡è¦æ±‚æ¥µé«˜çš„é‡è¦ä»»å‹™")
    print("=" * 80)

if __name__ == "__main__":
    # æ¸¬è©¦é…ç½®ç³»çµ±
    print_llm_configuration()
    print_budget_comparison()
    
    # æ¸¬è©¦å‰µå»ºä¸åŒé¡å‹çš„LLM
    print("\nğŸ§ª LLMå‰µå»ºæ¸¬è©¦:")
    for agent_type in ["literature_scout", "academic_writer"]:
        llm = LLMFactory.create_agent_llm(agent_type)
        config_name = AGENT_LLM_MAPPING[agent_type]
        print(f"  {agent_type}: {llm.model_name} (é…ç½®: {config_name})")
    
    # æ¸¬è©¦é ç®—å‹å¥½é…ç½®
    print("\nğŸ’° é ç®—å‹å¥½é…ç½®æ¸¬è©¦:")
    for tier in ["economy", "balanced", "premium"]:
        llm = LLMFactory.create_budget_conscious_llm("academic_writer", tier)
        print(f"  {tier} writer: {llm.model_name}")
    
    print("\nâœ… é…ç½®ç³»çµ±æ¸¬è©¦å®Œæˆï¼")
