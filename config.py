#!/usr/bin/env python3
"""
Veritas LLM Configuration Framework
æ¨¡å—åŒ–ã€å¯é…ç½®çš„LLMæ¶æ„ï¼Œæ”¯æŒä¸ºä¸åŒAgenté€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
"""

import os
from typing import Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

try:
    from langchain_openai import ChatOpenAI
except ImportError:
    # æµ‹è¯•æ¨¡å¼ï¼šåˆ›å»ºæ¨¡æ‹Ÿçš„ChatOpenAIç±»
    class ChatOpenAI:
        def __init__(self, model="gpt-4", temperature=0.1, **kwargs):
            self.model_name = model
            self.temperature = temperature
            self.kwargs = kwargs
        
        def __repr__(self):
            return f"ChatOpenAI(model={self.model_name}, temperature={self.temperature})"

# æ”¯æŒçš„LLMæä¾›å•†
class LLMProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    LOCAL = "local"

# æ¨¡å‹æ€§èƒ½çº§åˆ«
class ModelTier(Enum):
    BASIC = "basic"        # åŸºç¡€ä»»åŠ¡ï¼šæ ¼å¼è½¬æ¢ã€æ•°æ®æå–
    STANDARD = "standard"  # æ ‡å‡†ä»»åŠ¡ï¼šåˆ†æã€è§„åˆ’
    ADVANCED = "advanced"  # é«˜çº§ä»»åŠ¡ï¼šåˆ›ä½œã€ç¼–è¾‘
    PREMIUM = "premium"    # é¡¶çº§ä»»åŠ¡ï¼šå¤æ‚æ¨ç†ã€åˆ›æ–°

@dataclass
class LLMConfig:
    """LLMé…ç½®ç±»"""
    provider: LLMProvider
    model_name: str
    temperature: float = 0.1
    max_tokens: Optional[int] = None
    tier: ModelTier = ModelTier.STANDARD
    cost_per_token: float = 0.0  # æ¯tokenæˆæœ¬ï¼ˆç”¨äºæˆæœ¬ä¼˜åŒ–ï¼‰
    description: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "provider": self.provider.value,
            "model_name": self.model_name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "tier": self.tier.value,
            "cost_per_token": self.cost_per_token,
            "description": self.description
        }

# é¢„å®šä¹‰çš„LLMé…ç½®æ¨¡æ¿
LLM_CONFIGS = {
    # OpenAI Models
    "gpt-4-turbo": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-4-1106-preview",
        temperature=0.1,
        tier=ModelTier.PREMIUM,
        cost_per_token=0.00003,
        description="æœ€å¼ºå¤§çš„GPT-4æ¨¡å‹ï¼Œé€‚åˆå¤æ‚æ¨ç†å’Œåˆ›ä½œä»»åŠ¡"
    ),
    
    "gpt-4": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-4",
        temperature=0.1,
        tier=ModelTier.ADVANCED,
        cost_per_token=0.00003,
        description="å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬ï¼Œé€‚åˆç¼–è¾‘å’Œé«˜è´¨é‡å†™ä½œ"
    ),
    
    "gpt-4.1": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-4.1",
        temperature=0.1,
        tier=ModelTier.STANDARD,
        cost_per_token=0.000015,
        description="å½“å‰é»˜è®¤æ¨¡å‹ï¼Œæ€§èƒ½ç¨³å®šå¯é "
    ),
    
    "gpt-3.5-turbo": LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-3.5-turbo",
        temperature=0.1,
        tier=ModelTier.BASIC,
        cost_per_token=0.000002,
        description="é«˜æ€§ä»·æ¯”æ¨¡å‹ï¼Œé€‚åˆåŸºç¡€ä»»åŠ¡å’Œæ•°æ®å¤„ç†"
    ),
    
    # æœªæ¥å¯æ‰©å±•çš„é…ç½®
    "claude-3": LLMConfig(
        provider=LLMProvider.ANTHROPIC,
        model_name="claude-3-sonnet-20240229",
        temperature=0.1,
        tier=ModelTier.ADVANCED,
        cost_per_token=0.000015,
        description="Anthropic Claude 3ï¼Œæ“…é•¿æ¨ç†å’Œåˆ†æ"
    ),
}

# Agentä¸“ç”¨LLMé…ç½®æ˜ å°„
AGENT_LLM_MAPPING = {
    "literature_scout": "gpt-3.5-turbo",      # æ–‡çŒ®æœé›†ï¼šåŸºç¡€ä»»åŠ¡ï¼Œé‡è§†é€Ÿåº¦å’Œæˆæœ¬
    "synthesizer": "gpt-4.1",                 # ç ”ç©¶åˆ†æï¼šæ ‡å‡†ä»»åŠ¡ï¼Œéœ€è¦å‡†ç¡®æ€§
    "outline_planner": "gpt-4",               # å¤§çº²è§„åˆ’ï¼šé«˜çº§ä»»åŠ¡ï¼Œéœ€è¦é€»è¾‘æ€ç»´
    "academic_writer": "gpt-4-turbo",         # å­¦æœ¯å†™ä½œï¼šé¡¶çº§ä»»åŠ¡ï¼Œéœ€è¦åˆ›é€ åŠ›
    "editor": "gpt-4-turbo",                  # ç¼–è¾‘å®¡é˜…ï¼šé¡¶çº§ä»»åŠ¡ï¼Œéœ€è¦è¯­è¨€ç²¾é€š
    "citation_formatter": "gpt-3.5-turbo",   # å¼•æ–‡æ ¼å¼åŒ–ï¼šåŸºç¡€ä»»åŠ¡ï¼Œé‡è§†å‡†ç¡®æ€§
}

class LLMFactory:
    """LLMå·¥å‚ç±»ï¼Œè´Ÿè´£åˆ›å»ºå’Œç®¡ç†LLMå®ä¾‹"""
    
    @staticmethod
    def create_llm(config_name: str, **overrides) -> ChatOpenAI:
        """
        æ ¹æ®é…ç½®åç§°åˆ›å»ºLLMå®ä¾‹
        
        Args:
            config_name: é…ç½®åç§°
            **overrides: è¦†ç›–é»˜è®¤é…ç½®çš„å‚æ•°
            
        Returns:
            ChatOpenAIå®ä¾‹
        """
        if config_name not in LLM_CONFIGS:
            raise ValueError(f"æœªçŸ¥çš„LLMé…ç½®: {config_name}")
        
        config = LLM_CONFIGS[config_name]
        
        # åº”ç”¨è¦†ç›–å‚æ•°
        model_name = overrides.get("model_name", config.model_name)
        temperature = overrides.get("temperature", config.temperature)
        max_tokens = overrides.get("max_tokens", config.max_tokens)
        
        # ç›®å‰ä¸»è¦æ”¯æŒOpenAIï¼Œæœªæ¥å¯æ‰©å±•å…¶ä»–æä¾›å•†
        if config.provider == LLMProvider.OPENAI:
            llm_params = {
                "model": model_name,
                "temperature": temperature,
            }
            if max_tokens:
                llm_params["max_tokens"] = max_tokens
                
            return ChatOpenAI(**llm_params)
        else:
            raise NotImplementedError(f"æš‚ä¸æ”¯æŒæä¾›å•†: {config.provider}")
    
    @staticmethod
    def create_agent_llm(agent_type: str, **overrides) -> ChatOpenAI:
        """
        ä¸ºç‰¹å®šAgentç±»å‹åˆ›å»ºä¼˜åŒ–çš„LLMå®ä¾‹
        
        Args:
            agent_type: Agentç±»å‹
            **overrides: è¦†ç›–é»˜è®¤é…ç½®çš„å‚æ•°
            
        Returns:
            ChatOpenAIå®ä¾‹
        """
        if agent_type not in AGENT_LLM_MAPPING:
            print(f"âš ï¸ æœªæ‰¾åˆ°Agent '{agent_type}' çš„ä¸“ç”¨é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            config_name = "gpt-4.1"
        else:
            config_name = AGENT_LLM_MAPPING[agent_type]
        
        return LLMFactory.create_llm(config_name, **overrides)
    
    @staticmethod
    def get_model_info(config_name: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®ä¿¡æ¯"""
        if config_name not in LLM_CONFIGS:
            return {}
        
        config = LLM_CONFIGS[config_name]
        return config.to_dict()
    
    @staticmethod
    def list_available_models() -> Dict[str, str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®"""
        return {name: config.description for name, config in LLM_CONFIGS.items()}
    
    @staticmethod
    def estimate_cost(config_name: str, estimated_tokens: int) -> float:
        """ä¼°ç®—ä½¿ç”¨æˆæœ¬"""
        if config_name not in LLM_CONFIGS:
            return 0.0
        
        config = LLM_CONFIGS[config_name]
        return config.cost_per_token * estimated_tokens
    
    @staticmethod
    def get_cost_optimized_config(agent_type: str, complexity_level: str = "auto") -> str:
        """
        æ ¹æ®ä»»åŠ¡å¤æ‚åº¦å’Œæˆæœ¬è€ƒè™‘é€‰æ‹©æœ€ä¼˜é…ç½®
        
        Args:
            agent_type: Agentç±»å‹
            complexity_level: å¤æ‚åº¦çº§åˆ« ("low", "medium", "high", "auto")
            
        Returns:
            é…ç½®åç§°
        """
        # è‡ªåŠ¨æ ¹æ®Agentç±»å‹åˆ¤æ–­å¤æ‚åº¦
        if complexity_level == "auto":
            complexity_map = {
                "literature_scout": "low",      # æœç´¢ä»»åŠ¡ç›¸å¯¹ç®€å•
                "synthesizer": "medium",        # åˆ†æéœ€è¦ä¸€å®šèƒ½åŠ›
                "outline_planner": "high",      # è§„åˆ’éœ€è¦é«˜çº§æ¨ç†
                "academic_writer": "high",      # å†™ä½œéœ€è¦åˆ›é€ åŠ›
                "editor": "high",               # ç¼–è¾‘éœ€è¦è¯­è¨€ç²¾é€š
                "citation_formatter": "low"     # æ ¼å¼åŒ–ç›¸å¯¹æœºæ¢°
            }
            complexity_level = complexity_map.get(agent_type, "medium")
        
        # æ ¹æ®å¤æ‚åº¦é€‰æ‹©æœ€ç»æµçš„é…ç½®
        if complexity_level == "low":
            return "gpt-3.5-turbo"  # æœ€ç»æµ
        elif complexity_level == "medium":
            return "gpt-4.1"        # å¹³è¡¡
        else:  # high
            return "gpt-4-turbo"    # æœ€å¼ºèƒ½åŠ›
    
    @staticmethod
    def create_budget_conscious_llm(agent_type: str, budget_tier: str = "balanced") -> ChatOpenAI:
        """
        åˆ›å»ºé¢„ç®—å‹å¥½çš„LLMå®ä¾‹
        
        Args:
            agent_type: Agentç±»å‹
            budget_tier: é¢„ç®—çº§åˆ« ("economy", "balanced", "premium")
            
        Returns:
            ChatOpenAIå®ä¾‹
        """
        budget_configs = {
            "economy": {
                "literature_scout": "gpt-3.5-turbo",
                "synthesizer": "gpt-3.5-turbo", 
                "outline_planner": "gpt-4.1",
                "academic_writer": "gpt-4",
                "editor": "gpt-4",
                "citation_formatter": "gpt-3.5-turbo"
            },
            "balanced": AGENT_LLM_MAPPING,  # ä½¿ç”¨é»˜è®¤é…ç½®
            "premium": {
                "literature_scout": "gpt-4.1",
                "synthesizer": "gpt-4",
                "outline_planner": "gpt-4-turbo",
                "academic_writer": "gpt-4-turbo",
                "editor": "gpt-4-turbo",
                "citation_formatter": "gpt-4.1"
            }
        }
        
        config_name = budget_configs[budget_tier].get(agent_type, "gpt-4.1")
        return LLMFactory.create_llm(config_name)
    
    @staticmethod
    def compare_configurations() -> Dict[str, Any]:
        """æ¯”è¾ƒä¸åŒé…ç½®çš„æˆæœ¬å’Œæ€§èƒ½"""
        comparison = {}
        estimated_tokens = 1000  # å‡è®¾æ¯ä¸ªä»»åŠ¡1000ä¸ªtoken
        
        for tier in ["economy", "balanced", "premium"]:
            total_cost = 0
            tier_config = {}
            
            for agent_type in AGENT_LLM_MAPPING.keys():
                if tier == "economy":
                    config_name = LLMFactory.get_cost_optimized_config(agent_type, "low")
                elif tier == "premium":
                    config_name = LLMFactory.get_cost_optimized_config(agent_type, "high")
                else:
                    config_name = AGENT_LLM_MAPPING[agent_type]
                
                cost = LLMFactory.estimate_cost(config_name, estimated_tokens)
                total_cost += cost
                tier_config[agent_type] = {
                    "model": config_name,
                    "cost": cost,
                    "tier": LLM_CONFIGS[config_name].tier.value
                }
            
            comparison[tier] = {
                "total_cost": total_cost,
                "agents": tier_config,
                "cost_savings_vs_premium": 0  # å°†åœ¨ä¸‹é¢è®¡ç®—
            }
        
        # è®¡ç®—ç›¸å¯¹äºpremiumçš„æˆæœ¬èŠ‚çœ
        premium_cost = comparison["premium"]["total_cost"]
        for tier in comparison:
            if tier != "premium":
                savings = (premium_cost - comparison[tier]["total_cost"]) / premium_cost * 100
                comparison[tier]["cost_savings_vs_premium"] = savings
        
        return comparison

def print_llm_configuration():
    """æ‰“å°å½“å‰LLMé…ç½®æ¦‚è§ˆ"""
    print("\nğŸ§  Veritas å¤šå…ƒæ™ºèƒ½é…ç½®æ¦‚è§ˆ")
    print("=" * 70)
    print(f"{'Agentç±»å‹':<20} {'ä½¿ç”¨æ¨¡å‹':<15} {'æ€§èƒ½çº§åˆ«':<10} {'é¢„ä¼°æˆæœ¬/1K tokens'}")
    print("-" * 70)
    
    total_cost = 0
    for agent_type, config_name in AGENT_LLM_MAPPING.items():
        config = LLM_CONFIGS[config_name]
        cost = LLMFactory.estimate_cost(config_name, 1000)
        total_cost += cost
        print(f"{agent_type:<20} {config_name:<15} {config.tier.value:<10} ${cost:.4f}")
    
    print("-" * 70)
    print(f"{'æ€»ä¼°ç®—æˆæœ¬ (æ¯è½®)':<45} ${total_cost:.4f}")
    print("=" * 70)

def print_budget_comparison():
    """æ‰“å°ä¸åŒé¢„ç®—çº§åˆ«çš„æ¯”è¾ƒ"""
    print("\nğŸ’° é¢„ç®—çº§åˆ«å¯¹æ¯”åˆ†æ")
    print("=" * 80)
    
    comparison = LLMFactory.compare_configurations()
    
    for tier, data in comparison.items():
        print(f"\nğŸ“Š {tier.upper()} é…ç½®:")
        print(f"   æ€»æˆæœ¬: ${data['total_cost']:.4f} (æ¯è½®)")
        if data['cost_savings_vs_premium'] > 0:
            print(f"   èŠ‚çœ: {data['cost_savings_vs_premium']:.1f}% vs Premium")
        print("   Agenté…ç½®:")
        
        for agent, info in data['agents'].items():
            print(f"      {agent:<18}: {info['model']:<15} ({info['tier']} tier)")
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ å»ºè®®:")
    print("   â€¢ Economy: é€‚åˆå¤§è§„æ¨¡åº”ç”¨æˆ–é¢„ç®—ç´§å¼ çš„åœºæ™¯")
    print("   â€¢ Balanced: æ¨èé…ç½®ï¼Œæ€§èƒ½ä¸æˆæœ¬çš„æœ€ä½³å¹³è¡¡")  
    print("   â€¢ Premium: é€‚åˆå¯¹è´¨é‡è¦æ±‚æé«˜çš„é‡è¦ä»»åŠ¡")
    print("=" * 80)

if __name__ == "__main__":
    # æµ‹è¯•é…ç½®ç³»ç»Ÿ
    print_llm_configuration()
    print_budget_comparison()
    
    # æµ‹è¯•åˆ›å»ºä¸åŒç±»å‹çš„LLM
    print("\nğŸ§ª LLMåˆ›å»ºæµ‹è¯•:")
    for agent_type in ["literature_scout", "academic_writer"]:
        llm = LLMFactory.create_agent_llm(agent_type)
        config_name = AGENT_LLM_MAPPING[agent_type]
        print(f"  {agent_type}: {llm.model_name} (é…ç½®: {config_name})")
    
    # æµ‹è¯•é¢„ç®—å‹å¥½é…ç½®
    print("\nğŸ’° é¢„ç®—å‹å¥½é…ç½®æµ‹è¯•:")
    for tier in ["economy", "balanced", "premium"]:
        llm = LLMFactory.create_budget_conscious_llm("academic_writer", tier)
        print(f"  {tier} writer: {llm.model_name}")
    
    print("\nâœ… é…ç½®ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
